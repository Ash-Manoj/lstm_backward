{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to create a regional baseline model using an LSTM network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The following notebook contains the code to create, train, validate and test a rainfall-runoff model using a LSTM \n",
    "network architecture. \n",
    "The code's logic is heavily based on [Hy2DL](https://github.com/KIT-HYD/Hy2DL) and [Neural Hydrology](https://doi.org/10.21105/joss.04050)[1].\n",
    "\n",
    "**Inverse Model**\n",
    "\n",
    "We create a inverse model for the backward problem. The model uses the [Caravan GRDC extension](10.5281/zenodo.10074416). Here we run some experiments to see the value of adding discharge information for inferring precipitation patterns over Germany.\n",
    "\n",
    "**Authors**\n",
    "\n",
    "Ashish Manoj J (ashish.manoj@kit.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Optional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import List, Dict\n",
    "from itertools import groupby\n",
    "\n",
    "sys.path.append(\"../aux_functions\") # Here give the location of the aux_functions folder\n",
    "from functions_datasets import CARAVAN as DataBase # define what you import as DataBase! Also ensure that all the required Caravan datasets are downloaded and stored in a single folder!\n",
    "from functions_datasets import validate_samples \n",
    "from functions_training import nse_basin_averaged\n",
    "from functions_evaluation import rmse, nse\n",
    "from functions_aux import create_folder, set_random_seed, write_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. Initialize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to access the information\n",
    "path_entities = '../data/combined_europe.txt'\n",
    "path_data = '/pfs/data5/home/kit/iwu/as2023/dataset/01_europe_flood/Caravan/' # Caution! using absolute path here as I do not want to save the data together with the code\n",
    "\n",
    "# dynamic forcings and target\n",
    "dynamic_input = ['surface_net_solar_radiation_mean',\n",
    "                 'surface_net_thermal_radiation_mean','qobs_lead'] \n",
    "target = ['total_precipitation_sum']\n",
    "\n",
    "# static attributes that will be used\n",
    "static_input = ['area', 'p_mean','ele_mt_sav','frac_snow','pet_mm_syr']\n",
    "\n",
    "# time periods\n",
    "training_period = ['1980-10-01','2005-09-30']\n",
    "validation_period = ['2000-01-01','2005-12-31']\n",
    "\n",
    "model_hyper_parameters = {\n",
    "    \"input_size\": len(dynamic_input) + len(static_input),\n",
    "    \"no_of_layers\":1,  \n",
    "    \"seq_length\": 365,\n",
    "    \"hidden_size\": 64,\n",
    "    \"batch_size\":256,\n",
    "    \"no_of_epochs\": 5,             \n",
    "    \"drop_out\": 0.4, \n",
    "    \"learning_rate\": 0.001,\n",
    "    \"adapt_learning_rate_epoch\": 10,\n",
    "    \"adapt_gamma_learning_rate\": 0.5,\n",
    "    \"set_forget_gate\":3\n",
    "}\n",
    "\n",
    "# define random seed\n",
    "seed = 42\n",
    "# Name of the folder where the results will be stored \n",
    "path_save_folder = '../results/with_temperature'\n",
    "\n",
    "# colorblind friendly palette for plotting\n",
    "color_palette = {'observed': '#1f78b4','LSTM': '#ff7f00'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '../results/with_temperature' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Create folder to store the results\n",
    "create_folder(folder_path=path_save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Class to create the dataset object used to manage the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    \"\"\"Base data set class to load and preprocess data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dynamic_inputs : List[str]\n",
    "        name of variables used as dynamic series input in the lstm\n",
    "    static_inputs : List[str]\n",
    "        static inputs used as input in the lstm (e.g. catchment attributes)\n",
    "    target: List[str]\n",
    "        target variable(s)\n",
    "    sequence_length: int\n",
    "        sequence length used in the LSTM\n",
    "    time_period: List[str]\n",
    "        initial and final date (e.g. ['1987-10-01','1999-09-30']) of the time period of interest \n",
    "    path_entities: str\n",
    "        path to a txt file that contain the id of the entities (e.g. catchment`s ids) that will be analyzed\n",
    "    path_data: str\n",
    "        path to the folder were the data is stored\n",
    "    path_addional features: str\n",
    "        Optional parameter. Allows the option to add any arbitrary data that is not included in the standard data sets.\n",
    "        Path to a pickle file (or list of paths for multiple files), containing a dictionary with each key corresponding \n",
    "        to one basin id and the value is a date-time indexed pandas DataFrame.      \n",
    "    forcing: List[str]\n",
    "        For CAMELS-US dataset we should specificy which forcing data will be used (e.g. daymet, maurer, ndlas, etc.)\n",
    "    check_Nan: bool\n",
    "        Boolean that indicate if one should check of NaN values while processing the data\n",
    "    \"\"\"\n",
    "\n",
    "    #Function to initialize the data\n",
    "    def __init__(self, \n",
    "                 dynamic_input: List[str],\n",
    "                 static_input: List[str],\n",
    "                 target: List[str], \n",
    "                 sequence_length: int,\n",
    "                 time_period: List[str],\n",
    "                 path_entities: str,\n",
    "                 path_data: str,\n",
    "                 path_additional_features: str = '',\n",
    "                 forcings: List[str] = None,\n",
    "                 check_NaN:bool = True\n",
    "                 ):\n",
    "\n",
    "        # read and create variables\n",
    "        self.time_period = time_period # time period that is being considered\n",
    "        self.dynamic_input= dynamic_input  # dynamic forcings going as inputs of in the lstm\n",
    "        self.target = target  # target variable\n",
    "        self.sequence_length = sequence_length # sequence length\n",
    "\n",
    "        entities_ids = np.loadtxt(path_entities, dtype='str').tolist() \n",
    "        # save the cathments as a list even if there is just one\n",
    "        self.entities_ids = [entities_ids] if isinstance(entities_ids, str) else entities_ids # catchments\n",
    "\n",
    "        self.sequence_data = {} # store information that will be used to run the lstm\n",
    "        self.df_ts = {} # store processed dataframes for all basins\n",
    "        self.scaler = {} # information to standardize the data \n",
    "        self.basin_std = {} # std of the target variable of each basin (can be used later in the loss function)\n",
    "        self.valid_entities= [] # list of the elements that meet the criteria to be used by the lstm\n",
    "\n",
    "        # process the attributes\n",
    "        self.static_input = static_input # static attributes going as inputs to the lstm\n",
    "        if static_input:\n",
    "            self.df_attributes = self._load_attributes(path_data)\n",
    "\n",
    "        # process additional features that will be included in the inputs (optional) ---\n",
    "        if path_additional_features:\n",
    "            self.additional_features = self._load_additional_features(path_additional_features)\n",
    "        \n",
    "        # This loop goes through all the catchments. For each catchment in creates an entry in the dictionary\n",
    "        # self.sequence_data, where we will store the information that will be sent to the lstm\n",
    "        for id in self.entities_ids:\n",
    "            # load time series for specific catchment id\n",
    "            df_ts = self._load_data(path_data=path_data, catch_id=id)\n",
    "            # add additional features (optional)\n",
    "            if path_additional_features:\n",
    "                df_ts = pd.concat([df_ts, self.additional_features[id]], axis=1)\n",
    "            \n",
    "            # Defines the start date considering the offset due to sequence length. We want that, if possible, the start\n",
    "            # date is the first date of prediction.\n",
    "            start_date = pd.to_datetime(self.time_period[0],format=\"%Y-%m-%d\")\n",
    "            end_date = pd.to_datetime(self.time_period[1],format=\"%Y-%m-%d\")\n",
    "            freq = pd.infer_freq(df_ts.index)\n",
    "            warmup_start_date = start_date - (self.sequence_length-1)*pd.tseries.frequencies.to_offset(freq)\n",
    "            # filter dataframe for the period and variables of interest\n",
    "            df_ts = df_ts.loc[warmup_start_date:end_date, self.dynamic_input + self.target]\n",
    "            \n",
    "            # reindex the dataframe to assure continuos data between the start and end date of the time period. Missing \n",
    "            # data will be filled with NaN, so this will be taken care of later. \n",
    "            full_range = pd.date_range(start=warmup_start_date, end=end_date, freq=freq)\n",
    "            df_ts = df_ts.reindex(full_range)\n",
    "            \n",
    "            # checks for invalid samples due to NaN or insufficient sequence length\n",
    "            flag = validate_samples(x = df_ts.loc[:, self.dynamic_input].values, \n",
    "                                    y = df_ts.loc[:, self.target].values, \n",
    "                                    attributes = self.df_attributes.loc[id].values if static_input else None, \n",
    "                                    seq_length = self.sequence_length,\n",
    "                                    check_NaN = check_NaN\n",
    "                                    )\n",
    "            \n",
    "            # create a list that contain the indexes (basin, day) of the valid samples\n",
    "            valid_samples = np.argwhere(flag == 1)\n",
    "            self.valid_entities.extend([(id, int(f[0])) for f in valid_samples])\n",
    "            \n",
    "            # only store data if this basin has at least one valid sample in the given period\n",
    "            if valid_samples.size>0:\n",
    "                self.df_ts[id] = df_ts\n",
    "                \n",
    "                # create dictionary entry for the basin\n",
    "                self.sequence_data[id] = {}\n",
    "\n",
    "                # store the information of the basin in a nested dictionary\n",
    "                self.sequence_data[id]['x_d'] = torch.tensor(df_ts.loc[:, self.dynamic_input].values, dtype=torch.float32)\n",
    "                self.sequence_data[id]['y'] = torch.tensor(df_ts.loc[:, self.target].values, dtype=torch.float32)\n",
    "                if self.static_input:\n",
    "                    self.sequence_data[id]['x_s'] = torch.tensor(self.df_attributes.loc[id].values, dtype=torch.float32)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_entities)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        \"\"\"This function is used by PyTorch's dataloader to extract the information\"\"\"\n",
    "        basin, i = self.valid_entities[id]\n",
    "\n",
    "        # tensor of inputs\n",
    "        x_LSTM = self.sequence_data[basin]['x_d'][i-self.sequence_length+1:i+1, :]\n",
    "        if self.static_input:\n",
    "            x_s = self.sequence_data[basin]['x_s'].repeat(x_LSTM.shape[0],1)\n",
    "            x_LSTM = torch.cat([x_LSTM, x_s], dim=1)\n",
    "        \n",
    "        # Ttensor of outputs\n",
    "        y_obs = self.sequence_data[basin]['y'][i]\n",
    "\n",
    "        # optional also return the basin_std\n",
    "        if self.basin_std:\n",
    "            return x_LSTM, y_obs, self.basin_std[basin].unsqueeze(0)\n",
    "        else:\n",
    "            return x_LSTM, y_obs\n",
    "\n",
    "    def _load_attributes(self, path_data: str) -> pd.DataFrame:\n",
    "        \"\"\"Call the specific function that reads the static attributes information.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_data : str\n",
    "            path to the folder were the data is stored\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        df_attributes: pd.DataFrame\n",
    "            Dataframe containing the attributes of interest for the catchments of interest\n",
    "        \"\"\"\n",
    "        df_attributes = DataBase.read_attributes(path_data=path_data)\n",
    "        df_attributes = df_attributes.loc[self.entities_ids, self.static_input]\n",
    "        return df_attributes\n",
    "\n",
    "    def _load_data(self, path_data: str, catch_id:str) -> pd.DataFrame:\n",
    "        \"\"\"Call the specific function that reads a specific catchment timeseries into a dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_data : str\n",
    "            path to the folder were the data is stored.\n",
    "        catch_id : str\n",
    "            basin_id.\n",
    "        forcings : str\n",
    "            Can be e.g. 'daymet' or 'nldas', etc. Must match the folder names in the 'basin_mean_forcing' directory. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df: pd.DataFrame\n",
    "            Dataframe with the catchments` timeseries\n",
    "        \"\"\"\n",
    "        df_ts = DataBase.read_data(path_data=path_data, catch_id=catch_id)\n",
    "        return df_ts\n",
    "\n",
    "    def _load_additional_features(self, path_additional_features: str) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Read pickle dictionary containing additional features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_additional_features : str\n",
    "            Path to a pickle file (or list of paths for multiple files), containing a dictionary with each key \n",
    "            corresponding to one basin id and the value is a date-time indexed pandas DataFrame.   \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        additional_features: Dict[str, pd.DataFrame]\n",
    "            Dictionary where each key is a basin and each value is a date-time indexed pandas DataFrame with the \n",
    "            additional features\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(path_additional_features, \"rb\") as file:\n",
    "            additional_features = pickle.load(file)\n",
    "        return additional_features\n",
    "  \n",
    "    def calculate_basin_std(self):\n",
    "        \"\"\"Fill the self.basin_std dictionary with the standard deviation of the target variables for each basin\"\"\"\n",
    "        for id, data in self.sequence_data.items():\n",
    "            self.basin_std[id] = torch.tensor(np.nanstd(data['y'].numpy()), dtype=torch.float32)\n",
    "    \n",
    "    def calculate_global_statistics(self,path_save_scaler:Optional[str] = ''):\n",
    "        \"\"\"Fill the self.scalar dictionary \n",
    "        \n",
    "        The function calculates the global mean and standard deviation of the dynamic inputs, target variables and \n",
    "        static attributes, and store the in a dictionary. It will be used later to standardize used in the LSTM. This\n",
    "        function should be called only in training period. \n",
    "        \"\"\"\n",
    "        global_x = np.vstack([df.loc[:, self.dynamic_input].values for df in self.df_ts.values()])\n",
    "        self.scaler['x_d_mean'] = torch.tensor(np.nanmean(global_x, axis=0), dtype=torch.float32)\n",
    "        self.scaler['x_d_std'] = torch.tensor(np.nanstd(global_x, axis=0), dtype=torch.float32)\n",
    "        del global_x\n",
    "\n",
    "        global_y = np.vstack([df.loc[:, self.target].values for df in self.df_ts.values()])\n",
    "        self.scaler['y_mean'] = torch.tensor(np.nanmean(global_y, axis=0), dtype=torch.float32)\n",
    "        self.scaler['y_std'] = torch.tensor(np.nanstd(global_y, axis=0), dtype=torch.float32)\n",
    "        del global_y\n",
    "\n",
    "        if self.static_input:\n",
    "            self.scaler['x_s_mean'] = torch.tensor(self.df_attributes.mean().values, dtype= torch.float32)\n",
    "            self.scaler['x_s_std'] = torch.tensor(self.df_attributes.std().values, dtype= torch.float32)\n",
    "\n",
    "        if path_save_scaler: #save the results in a pickle file\n",
    "            with open(path_save_scaler+'/scaler.pickle', 'wb') as f:\n",
    "                pickle.dump(self.scaler, f)\n",
    "\n",
    "    \n",
    "    def standardize_data(self, standardize_output:bool=True):\n",
    "        \"\"\"Standardize the data used in the LSTM. \n",
    "\n",
    "        The function standardize the data contained in the self.sequence_data dictionary \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        standardize_output : bool\n",
    "            Boolean to define if the output should be standardize or not. \n",
    "        \"\"\"\n",
    "        for basin in self.sequence_data.values():\n",
    "            # Standardize input\n",
    "            basin['x_d'] = (basin['x_d'] - self.scaler['x_d_mean']) / self.scaler['x_d_std']\n",
    "            if self.static_input:\n",
    "                basin['x_s'] = (basin['x_s'] - self.scaler['x_s_mean']) / self.scaler['x_s_std']\n",
    "            # Standardize output\n",
    "            if standardize_output:\n",
    "                basin['y'] = (basin['y'] - self.scaler['y_mean']) / self.scaler['y_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Create the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset training\n",
    "training_dataset = BaseDataset(dynamic_input=dynamic_input,\n",
    "                               static_input=static_input,\n",
    "                               target=target,\n",
    "                               sequence_length=model_hyper_parameters['seq_length'],\n",
    "                               time_period=training_period,\n",
    "                               path_entities=path_entities,\n",
    "                               path_data=path_data,\n",
    "                               check_NaN=True)\n",
    "\n",
    "training_dataset.calculate_basin_std()\n",
    "training_dataset.calculate_global_statistics(path_save_scaler=path_save_folder) # the global statistics are calculated in the training period!\n",
    "training_dataset.standardize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset validation\n",
    "validation_dataset = BaseDataset(dynamic_input=dynamic_input,\n",
    "                                 static_input=static_input,\n",
    "                                 target=target,\n",
    "                                 sequence_length=model_hyper_parameters['seq_length'],\n",
    "                                 time_period=validation_period,\n",
    "                                 path_entities=path_entities,\n",
    "                                 path_data=path_data,\n",
    "                                 check_NaN=False)\n",
    "\n",
    "validation_dataset.scaler = training_dataset.scaler # read the global statisctics calculated in the training period\n",
    "validation_dataset.standardize_data(standardize_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4. Create the different dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches in training:  53491\n",
      "x_lstm: torch.Size([256, 365, 8]) | y: torch.Size([256, 1]) | per_basin_target_std: torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader for training data.\n",
    "train_loader = DataLoader(training_dataset, \n",
    "                          batch_size=model_hyper_parameters['batch_size'],\n",
    "                          shuffle=True,\n",
    "                          drop_last = True)\n",
    "\n",
    "print('Batches in training: ', len(train_loader))\n",
    "x_lstm, y, per_basin_target_std = next(iter(train_loader))\n",
    "print(f'x_lstm: {x_lstm.shape} | y: {y.shape} | per_basin_target_std: {per_basin_target_std.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches in validation:  1804\n",
      "x_lstm: torch.Size([2192, 365, 8]) | y: torch.Size([2192, 1])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader for validation data.\n",
    "validation_batches=[[index for index, _ in group] for _ , group in groupby(enumerate(validation_dataset.valid_entities), \n",
    "                                                                           lambda x: x[1][0])] # each basin is one batch\n",
    "\n",
    "validation_loader = DataLoader(dataset=validation_dataset,\n",
    "                               batch_sampler=validation_batches)\n",
    "\n",
    "# see if the batches are loaded correctly\n",
    "print('Batches in validation: ', len(validation_loader))\n",
    "x_lstm, y= next(iter(validation_loader))\n",
    "print(f'x_lstm: {x_lstm.shape} | y: {y.shape}')\n",
    "\n",
    "# create some lists with the valid basins and the valid entities per basin that will help later to organize the data\n",
    "valid_basins = [next(group)[0] for key, group in groupby(validation_dataset.valid_entities, key=lambda x: x[0])]\n",
    "valid_entity_per_basin = [[id for _, id in group] for key, group in groupby(validation_dataset.valid_entities, \n",
    "                                                                            key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5. Define LSTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device (this will use all available GPUs)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Cuda_LSTM(nn.Module):\n",
    "    def __init__(self, model_hyper_parameters):\n",
    "        super().__init__()\n",
    "        self.num_features = model_hyper_parameters['input_size']\n",
    "        self.hidden_units = model_hyper_parameters['hidden_size']\n",
    "        self.num_layers = model_hyper_parameters['no_of_layers']\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = model_hyper_parameters['input_size'], \n",
    "                            hidden_size = model_hyper_parameters['hidden_size'], \n",
    "                            batch_first = True,\n",
    "                            num_layers = model_hyper_parameters['no_of_layers'])\n",
    "\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(model_hyper_parameters['drop_out'])\n",
    "        self.linear = nn.Linear(in_features=model_hyper_parameters['hidden_size'], out_features=1)\n",
    "           \n",
    "    def forward(self, x):\n",
    "        # initialize hidden state with zeros\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units, requires_grad=True, dtype=torch.float32, \n",
    "                         device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units, requires_grad=True, dtype=torch.float32,\n",
    "                         device=x.device)\n",
    "        \n",
    "        out, (hn_1, cn_1) = self.lstm(x, (h0, c0))\n",
    "        out = out[:,-1,:] # sequence to one\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(path, 'checkpoint.pth'))\n",
    "    print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, path):\n",
    "    checkpoint = torch.load(os.path.join(path, 'checkpoint.pth'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Checkpoint loaded: starting from epoch {start_epoch}\")\n",
    "    return start_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.2.140)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#run only if cuDNN version incompatibility in cluster\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6 Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded: starting from epoch 4\n",
      "Checkpoint saved at epoch 5\n",
      "Epoch: 5  | Loss training: 0.393  | NSE validation: 0.612  | LR:0.00100  | Training time: 1749.1  s\n",
      "Total training time: 1749.1  s\n"
     ]
    }
   ],
   "source": [
    "# construct model\n",
    "set_random_seed(seed=seed)\n",
    "lstm_model = Cuda_LSTM(model_hyper_parameters).to(device)\n",
    "\n",
    "# Wrap the model with DataParallel\n",
    "lstm_model = nn.DataParallel(lstm_model)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=model_hyper_parameters[\"learning_rate\"])\n",
    "\n",
    "# define learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=model_hyper_parameters[\"adapt_learning_rate_epoch\"], gamma=model_hyper_parameters[\"adapt_gamma_learning_rate\"])\n",
    "\n",
    "# Access the LSTM layer directly from the model\n",
    "lstm_layer = lstm_model.module.lstm \n",
    "\n",
    "# Set the forget gate bias of the LSTM layer\n",
    "forget_gate_bias = model_hyper_parameters[\"set_forget_gate\"]\n",
    "hidden_size = model_hyper_parameters[\"hidden_size\"]\n",
    "lstm_layer.bias_hh_l0.data[hidden_size:2 * hidden_size] = forget_gate_bias\n",
    "\n",
    "checkpoint_path = path_save_folder\n",
    "start_epoch = 0\n",
    "if os.path.exists(os.path.join(checkpoint_path, 'checkpoint.pth')):\n",
    "    start_epoch = load_checkpoint(lstm_model, optimizer, scheduler, checkpoint_path)\n",
    "\n",
    "training_time = time.time()\n",
    "# Loop through the different epochs\n",
    "for epoch in range(start_epoch, model_hyper_parameters[\"no_of_epochs\"]):\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = 0\n",
    "    # Training ----------------------------------------------------------------------\n",
    "    lstm_model.train()\n",
    "    for (x_lstm, y, per_basin_target_std) in train_loader: \n",
    "        optimizer.zero_grad() # sets gradients of weights and bias to zero\n",
    "\n",
    "        # Move data to the device\n",
    "        x_lstm = x_lstm.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_sim = lstm_model(x_lstm) # forward call\n",
    "        \n",
    "        loss = F.mse_loss(y_sim, y)\n",
    "        \n",
    "        loss.backward() # backpropagates\n",
    "        optimizer.step() # update weights\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # remove from cuda\n",
    "        del x_lstm, y, y_sim, per_basin_target_std\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # average loss training   \n",
    "    average_loss_training = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation ----------------------------------------------------------------------\n",
    "    lstm_model.eval()\n",
    "    validation_results = {}\n",
    "    with torch.no_grad():\n",
    "        for i, (x_lstm, y) in enumerate(validation_loader): \n",
    "             # Move data to the device\n",
    "            x_lstm = x_lstm.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # run LSTM\n",
    "            y_sim = lstm_model(x_lstm) # forward call\n",
    "            \n",
    "            # scale back prediction\n",
    "            y_sim = y_sim * validation_dataset.scaler['y_std'].to(device) + validation_dataset.scaler['y_mean'].to(device)\n",
    "\n",
    "            # join results in a dataframe and store them in a dictionary (is easier to plot later)\n",
    "            df_ts = validation_dataset.df_ts[valid_basins[i]].iloc[valid_entity_per_basin[i]]\n",
    "            df_new = pd.DataFrame(data={'y_obs': y.flatten().cpu().detach().numpy(), 'y_sim': y_sim.flatten().cpu().detach().numpy()}, index=df_ts.index)\n",
    "            df_ts = pd.concat([df_ts, df_new], axis=1)\n",
    "            df_ts = df_ts.filter(['y_obs', 'y_sim'])\n",
    "            validation_results[valid_basins[i]] = df_ts\n",
    "            \n",
    "            # remove from cuda\n",
    "            del x_lstm, y, y_sim\n",
    "            torch.cuda.empty_cache()       \n",
    "            \n",
    "        # average loss validation\n",
    "        loss_validation = nse(df_results=validation_results)\n",
    "\n",
    "    # save model and optimizer state after every epoch\n",
    "    save_checkpoint(lstm_model, optimizer, scheduler, epoch, checkpoint_path)\n",
    "    path_saved_model = path_save_folder+'/epoch_' + str(epoch+1)\n",
    "    torch.save(lstm_model.state_dict(), path_saved_model)\n",
    "            \n",
    "    # print epoch report\n",
    "    epoch_training_time = time.time() - epoch_start_time\n",
    "    LR = optimizer.param_groups[0]['lr']\n",
    "    report = f'Epoch: {epoch + 1:<2} | Loss training: {\"%.3f \" % (average_loss_training)} | NSE validation: {\"%.3f \" % (loss_validation)} | LR:{\"%.5f \" % (LR)} | Training time: {\"%.1f \" % (epoch_training_time)} s'\n",
    "    print(report)\n",
    "    # save epoch report in txt file\n",
    "    write_report(file_path=path_save_folder + '/run_progress.txt', text=report)\n",
    "    # modify learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# print total report\n",
    "total_training_time = time.time() - training_time\n",
    "report = f'Total training time: {\"%.1f \" % (total_training_time)} s'\n",
    "print(report)\n",
    "# save total report in txt file\n",
    "write_report(file_path=path_save_folder + '/run_progress.txt', text=report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 7. Test LSTM (All catchments, 2011-2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches in testing:  1804\n",
      "x_lstm: torch.Size([5479, 365, 8]) | y: torch.Size([5479, 1])\n"
     ]
    }
   ],
   "source": [
    "# Dataset testing\n",
    "testing_period = ['2006-01-01','2020-12-31']\n",
    "test_dataset = BaseDataset(dynamic_input=dynamic_input,\n",
    "                           static_input=static_input,\n",
    "                           target=target,\n",
    "                           sequence_length=model_hyper_parameters['seq_length'],\n",
    "                           time_period=testing_period,\n",
    "                           path_entities=path_entities,\n",
    "                           path_data=path_data,\n",
    "                           check_NaN=False)\n",
    "# We can read a previously stored scaler or use the one from the training dataset we just generated\n",
    "#scaler = training_dataset.scaler\n",
    "with open(path_save_folder + \"/scaler.pickle\", \"rb\") as file:\n",
    "    scaler = pickle.load(file)\n",
    "    \n",
    "test_dataset.scaler = scaler # read the global statisctics calculated in the training period\n",
    "test_dataset.standardize_data(standardize_output=False)\n",
    "\n",
    "# DataLoader for testing data.\n",
    "test_batches=[[index for index, _ in group] for _ , group in groupby(enumerate(test_dataset.valid_entities), \n",
    "                                                                     lambda x: x[1][0])]\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_sampler=test_batches)\n",
    "\n",
    "# see if the batches are loaded correctly\n",
    "print('Batches in testing: ', len(test_loader))\n",
    "x_lstm, y= next(iter(test_loader))\n",
    "print(f'x_lstm: {x_lstm.shape} | y: {y.shape}')\n",
    "\n",
    "# create some lists with the valid basins and the valid entities per basin that will help later to organize the data\n",
    "valid_basins_testing = [next(group)[0] for key, group in groupby(test_dataset.valid_entities, key=lambda x: x[0])]\n",
    "valid_entity_per_basin_testing = [[id for _, id in group] for key, group in groupby(test_dataset.valid_entities, \n",
    "                                                                                    key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.eval()\n",
    "test_results = {}\n",
    "# Testing----------------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    for i, (x_lstm, y) in enumerate(test_loader):\n",
    "        # run LSTM\n",
    "        y_sim = lstm_model(x_lstm.to(device)) # forward call\n",
    "        \n",
    "        # scale back prediction\n",
    "        y_sim = y_sim* test_dataset.scaler['y_std'].to(device) + test_dataset.scaler['y_mean'].to(device)\n",
    "\n",
    "        # join results in a dataframe and store them in a dictionary (is easier to plot later)\n",
    "        df_ts = test_dataset.df_ts[valid_basins_testing[i]].iloc[valid_entity_per_basin_testing[i]]\n",
    "        df_new = pd.DataFrame(data={'y_obs': y.flatten().cpu().detach().numpy(), \n",
    "                                    'y_sim': y_sim.flatten().cpu().detach().numpy()}, index=df_ts.index)\n",
    "        df_ts = pd.concat([df_ts, df_new], axis=1)\n",
    "        df_ts = df_ts.filter(['y_obs', 'y_sim'])\n",
    "        test_results[valid_basins_testing[i]] = df_ts\n",
    "        \n",
    "        # remove from cuda\n",
    "        del x_lstm, y, y_sim\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 8. Initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss testing\n",
    "loss_testing = nse(df_results=test_results, average=False)\n",
    "df_NSE = pd.DataFrame(data={'basin_id': valid_basins_testing, 'NSE': np.round(loss_testing,3)})\n",
    "df_NSE = df_NSE.set_index('basin_id')\n",
    "df_NSE.to_csv(path_save_folder+'/NSE.csv', index=True, header=True)\n",
    "\n",
    "# Simulated and observed results\n",
    "y_sim = pd.concat([pd.DataFrame({key: value['y_sim']}) for key, value in test_results.items()], axis=1)\n",
    "y_obs = pd.concat([pd.DataFrame({key: value['y_obs']}) for key, value in test_results.items()], axis=1)\n",
    "\n",
    "# Set index same as last dataframe in loop (because of how the dataframes where constructed all have the same indexes)\n",
    "y_sim = y_sim.set_index(list(test_results.values())[0]['y_sim'].index)\n",
    "y_obs = y_obs.set_index(list(test_results.values())[0]['y_obs'].index)\n",
    "\n",
    "# Export the results\n",
    "y_sim.to_csv(path_save_folder+'/y_sim.csv', index=True, header=True)\n",
    "y_obs.to_csv(path_save_folder+'/y_obs.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the histogram\n",
    "plt.hist(df_NSE['NSE'], bins=[0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1])\n",
    "\n",
    "# Add NSE statistics to the plot\n",
    "plt.text(0.01, 0.8, f'Mean: {\"%.2f\" % df_NSE[\"NSE\"].mean():>7}\\nMedian: {\"%.2f\" % df_NSE[\"NSE\"].median():>0}\\nMax: {\"%.2f\" % df_NSE[\"NSE\"].max():>9}\\nMin: {\"%.2f\" % df_NSE[\"NSE\"].min():>10}',\n",
    "         transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "# Format plot\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.xlabel('NSE', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "plt.title('NSE Histogram', fontsize=16, fontweight='bold')\n",
    "#plt.savefig(save_folder+'/NSE_LSTM_Histogram.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "42b7dc197ee81dd2f6541889b0e14556b882d218c1e7c97db94bc0f7b191f034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
