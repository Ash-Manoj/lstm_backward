{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7352572-d6c5-4c06-a522-dc16e4623e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_one_more_testcase.py\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import logging\n",
    "from itertools import groupby\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# --- IMPORTANT: same aux_functions imports as your training script ---\n",
    "sys.path.append(\"../aux_functions\")  # adjust if needed\n",
    "from functions_datasets import CARAVAN as DataBase\n",
    "from functions_datasets import validate_samples\n",
    "from functions_evaluation import nse\n",
    "from functions_aux import create_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca870fa-56f3-4333-8d50-2e03909f384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '../results/with_discharge_haicore/seed_77584/one_more_testcase' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# CONFIG (edit these)\n",
    "# --------------------------\n",
    "\n",
    "# Folder of an EXISTING run (seed folder) that already contains scaler.pickle and checkpoint.pth\n",
    "RUN_FOLDER = \"../results/with_discharge_haicore/seed_77584\"  # <-- change to your seed folder\n",
    "\n",
    "# If you prefer loading a specific epoch file (your script saved it as RUN_FOLDER/epoch_X), set this.\n",
    "# Otherwise the script will load RUN_FOLDER/checkpoint.pth.\n",
    "EPOCH_STATE_PATH = \"\"  # e.g. \"../results/.../seed_77584/epoch_5\"  (leave \"\" to use checkpoint.pth)\n",
    "\n",
    "# Data\n",
    "PATH_DATA = \"../../datasets\"  # same as your script\n",
    "TESTING_PERIOD = [\"1993-01-01\", \"1995-12-31\"]\n",
    "\n",
    "# One more test case: basin ids you want to test now\n",
    "BASIN_IDS_TO_TEST = [\"neck_1\"]  # put your basin id(s) here\n",
    "\n",
    "# Same variable lists as your script\n",
    "DYNAMIC_INPUT = [\n",
    "    \"temperature_2m_mean\",\n",
    "    \"surface_net_solar_radiation_mean\",\n",
    "    \"surface_net_thermal_radiation_mean\",\n",
    "    \"qobs_lead\",\n",
    "    \"total_precipitation_sum\",\n",
    "]\n",
    "STATIC_INPUT = [\"area\", \"ele_mt_sav\", \"frac_snow\", \"pet_mm_syr\"]\n",
    "TARGET = [\"eobs_new\"]\n",
    "\n",
    "MODEL_HYPER_PARAMETERS = {\n",
    "    \"input_size\": len(DYNAMIC_INPUT) + len(STATIC_INPUT),\n",
    "    \"no_of_layers\": 1,\n",
    "    \"seq_length\": 365,\n",
    "    \"hidden_size\": 64,\n",
    "    \"batch_size\": 256,  # not critical here (we use batch_sampler per basin)\n",
    "    \"drop_out\": 0.4,\n",
    "    \"set_forget_gate\": 3,\n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Output\n",
    "OUT_DIR = os.path.join(RUN_FOLDER, \"one_more_testcase\")\n",
    "create_folder(OUT_DIR)\n",
    "\n",
    "# Logging\n",
    "log_file = os.path.join(OUT_DIR, \"one_more_testcase.log\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    filemode=\"w\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    ")\n",
    "logging.info(\"Starting one-more-testcase script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae7c9d2-6e0b-4cac-b9ef-5c4f15e49fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Dataset + Model (copied/minimized from your file)\n",
    "# --------------------------\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dynamic_input: List[str],\n",
    "        static_input: List[str],\n",
    "        target: List[str],\n",
    "        sequence_length: int,\n",
    "        time_period: List[str],\n",
    "        path_entities: str,\n",
    "        path_data: str,\n",
    "        path_additional_features: str = \"\",\n",
    "        forcings: List[str] = None,\n",
    "        check_NaN: bool = True,\n",
    "    ):\n",
    "        self.time_period = time_period\n",
    "        self.dynamic_input = dynamic_input\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        entities_ids = np.loadtxt(path_entities, dtype=\"str\").tolist()\n",
    "        self.entities_ids = [entities_ids] if isinstance(entities_ids, str) else entities_ids\n",
    "\n",
    "        self.sequence_data = {}\n",
    "        self.df_ts = {}\n",
    "        self.scaler = {}\n",
    "        self.basin_std = {}\n",
    "        self.valid_entities = []\n",
    "\n",
    "        self.static_input = static_input\n",
    "        if static_input:\n",
    "            self.df_attributes = self._load_attributes(path_data)\n",
    "\n",
    "        if path_additional_features:\n",
    "            self.additional_features = self._load_additional_features(path_additional_features)\n",
    "\n",
    "        for basin_id in self.entities_ids:\n",
    "            df_ts = self._load_data(path_data=path_data, catch_id=basin_id)\n",
    "\n",
    "            if path_additional_features:\n",
    "                df_ts = pd.concat([df_ts, self.additional_features[basin_id]], axis=1)\n",
    "\n",
    "            start_date = pd.to_datetime(self.time_period[0], format=\"%Y-%m-%d\")\n",
    "            end_date = pd.to_datetime(self.time_period[1], format=\"%Y-%m-%d\")\n",
    "            freq = pd.infer_freq(df_ts.index)\n",
    "            warmup_start_date = start_date - (self.sequence_length - 1) * pd.tseries.frequencies.to_offset(freq)\n",
    "\n",
    "            df_ts = df_ts.loc[warmup_start_date:end_date, self.dynamic_input + self.target]\n",
    "\n",
    "            full_range = pd.date_range(start=warmup_start_date, end=end_date, freq=freq)\n",
    "            df_ts = df_ts.reindex(full_range)\n",
    "\n",
    "            flag = validate_samples(\n",
    "                x=df_ts.loc[:, self.dynamic_input].values,\n",
    "                y=df_ts.loc[:, self.target].values,\n",
    "                attributes=self.df_attributes.loc[basin_id].values if static_input else None,\n",
    "                seq_length=self.sequence_length,\n",
    "                check_NaN=check_NaN,\n",
    "            )\n",
    "\n",
    "            valid_samples = np.argwhere(flag == 1)\n",
    "            self.valid_entities.extend([(basin_id, int(f[0])) for f in valid_samples])\n",
    "\n",
    "            if valid_samples.size > 0:\n",
    "                self.df_ts[basin_id] = df_ts\n",
    "                self.sequence_data[basin_id] = {}\n",
    "                self.sequence_data[basin_id][\"x_d\"] = torch.tensor(\n",
    "                    df_ts.loc[:, self.dynamic_input].values, dtype=torch.float32\n",
    "                )\n",
    "                self.sequence_data[basin_id][\"y\"] = torch.tensor(\n",
    "                    df_ts.loc[:, self.target].values, dtype=torch.float32\n",
    "                )\n",
    "                if self.static_input:\n",
    "                    self.sequence_data[basin_id][\"x_s\"] = torch.tensor(\n",
    "                        self.df_attributes.loc[basin_id].values, dtype=torch.float32\n",
    "                    )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_entities)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        basin, i = self.valid_entities[idx]\n",
    "        x_lstm = self.sequence_data[basin][\"x_d\"][i - self.sequence_length + 1 : i + 1, :]\n",
    "        if self.static_input:\n",
    "            x_s = self.sequence_data[basin][\"x_s\"].repeat(x_lstm.shape[0], 1)\n",
    "            x_lstm = torch.cat([x_lstm, x_s], dim=1)\n",
    "        y_obs = self.sequence_data[basin][\"y\"][i]\n",
    "        return x_lstm, y_obs\n",
    "\n",
    "    def _load_attributes(self, path_data: str) -> pd.DataFrame:\n",
    "        df_attributes = DataBase.read_attributes(path_data=path_data)\n",
    "        df_attributes = df_attributes.loc[self.entities_ids, self.static_input]\n",
    "        return df_attributes\n",
    "\n",
    "    def _load_data(self, path_data: str, catch_id: str) -> pd.DataFrame:\n",
    "        return DataBase.read_data(path_data=path_data, catch_id=catch_id)\n",
    "\n",
    "    def _load_additional_features(self, path_additional_features: str) -> Dict[str, pd.DataFrame]:\n",
    "        with open(path_additional_features, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def standardize_data(self, standardize_output: bool = True):\n",
    "        for basin in self.sequence_data.values():\n",
    "            basin[\"x_d\"] = (basin[\"x_d\"] - self.scaler[\"x_d_mean\"]) / self.scaler[\"x_d_std\"]\n",
    "            if self.static_input:\n",
    "                basin[\"x_s\"] = (basin[\"x_s\"] - self.scaler[\"x_s_mean\"]) / self.scaler[\"x_s_std\"]\n",
    "            if standardize_output:\n",
    "                basin[\"y\"] = (basin[\"y\"] - self.scaler[\"y_mean\"]) / self.scaler[\"y_std\"]\n",
    "\n",
    "\n",
    "class Cuda_LSTM(nn.Module):\n",
    "    def __init__(self, model_hyper_parameters):\n",
    "        super().__init__()\n",
    "        self.hidden_units = model_hyper_parameters[\"hidden_size\"]\n",
    "        self.num_layers = model_hyper_parameters[\"no_of_layers\"]\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=model_hyper_parameters[\"input_size\"],\n",
    "            hidden_size=model_hyper_parameters[\"hidden_size\"],\n",
    "            batch_first=True,\n",
    "            num_layers=model_hyper_parameters[\"no_of_layers\"],\n",
    "        )\n",
    "        self.dropout = nn.Dropout(model_hyper_parameters[\"drop_out\"])\n",
    "        self.linear = nn.Linear(model_hyper_parameters[\"hidden_size\"], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units, device=x.device, dtype=torch.float32)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units, device=x.device, dtype=torch.float32)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        return self.linear(out)\n",
    "\n",
    "\n",
    "def _strip_module_prefix(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    # Helps loading whether weights were saved from DataParallel or not\n",
    "    if not any(k.startswith(\"module.\") for k in state_dict.keys()):\n",
    "        return state_dict\n",
    "    return {k.replace(\"module.\", \"\", 1): v for k, v in state_dict.items()}\n",
    "\n",
    "\n",
    "def load_model(run_folder: str) -> nn.Module:\n",
    "    model = Cuda_LSTM(MODEL_HYPER_PARAMETERS).to(DEVICE)\n",
    "\n",
    "    # Apply same forget-gate bias tweak (safe even for inference-only)\n",
    "    lstm_layer = model.lstm\n",
    "    hidden_size = MODEL_HYPER_PARAMETERS[\"hidden_size\"]\n",
    "    lstm_layer.bias_hh_l0.data[hidden_size : 2 * hidden_size] = MODEL_HYPER_PARAMETERS[\"set_forget_gate\"]\n",
    "\n",
    "    # Load weights\n",
    "    if EPOCH_STATE_PATH:\n",
    "        raw = torch.load(EPOCH_STATE_PATH, map_location=DEVICE)\n",
    "        # epoch_X was saved as state_dict in your script\n",
    "        try:\n",
    "            model.load_state_dict(_strip_module_prefix(raw), strict=True)\n",
    "        except RuntimeError:\n",
    "            # maybe it includes module.* keys already suitable for DataParallel\n",
    "            dp = nn.DataParallel(model)\n",
    "            dp.load_state_dict(raw, strict=True)\n",
    "            model = dp.module\n",
    "    else:\n",
    "        ckpt_path = os.path.join(run_folder, \"checkpoint.pth\")\n",
    "        ckpt = torch.load(ckpt_path, map_location=DEVICE)\n",
    "        sd = ckpt[\"model_state_dict\"]\n",
    "        try:\n",
    "            model.load_state_dict(_strip_module_prefix(sd), strict=True)\n",
    "        except RuntimeError:\n",
    "            dp = nn.DataParallel(model)\n",
    "            dp.load_state_dict(sd, strict=True)\n",
    "            model = dp.module\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def write_entities_file(out_dir: str, basin_ids: List[str]) -> str:\n",
    "    path = os.path.join(out_dir, \"entities_one_more_test.txt\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for bid in basin_ids:\n",
    "            f.write(f\"{bid}\\n\")\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "507af66f-2bb3-46c9-bbbb-d0e1477e2b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '../results/with_discharge_haicore/seed_77584/one_more_testcase/predictions' created successfully.\n",
      "Done.\n",
      "Saved results in: ../results/with_discharge_haicore/seed_77584/one_more_testcase\n",
      "            NSE\n",
      "basin_id       \n",
      "neck_1    0.614\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1) Load scaler\n",
    "    scaler_path = os.path.join(RUN_FOLDER, \"scaler.pickle\")\n",
    "    with open(scaler_path, \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "    logging.info(\"Loaded scaler.pickle\")\n",
    "\n",
    "    # 2) Build dataset for your new test basin(s)\n",
    "    path_entities = write_entities_file(OUT_DIR, BASIN_IDS_TO_TEST)\n",
    "    test_dataset = BaseDataset(\n",
    "        dynamic_input=DYNAMIC_INPUT,\n",
    "        static_input=STATIC_INPUT,\n",
    "        target=TARGET,\n",
    "        sequence_length=MODEL_HYPER_PARAMETERS[\"seq_length\"],\n",
    "        time_period=TESTING_PERIOD,\n",
    "        path_entities=path_entities,\n",
    "        path_data=PATH_DATA,\n",
    "        check_NaN=False,\n",
    "    )\n",
    "    test_dataset.scaler = scaler\n",
    "    test_dataset.standardize_data(standardize_output=False)  # IMPORTANT: same as your testing part\n",
    "\n",
    "    # Basin-wise batches (same logic as your script)\n",
    "    test_batches = [\n",
    "        [index for index, _ in group]\n",
    "        for _, group in groupby(enumerate(test_dataset.valid_entities), lambda x: x[1][0])\n",
    "    ]\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_sampler=test_batches)\n",
    "    logging.info(f\"Batches in testing: {len(test_loader)}\")\n",
    "\n",
    "    valid_basins = [next(group)[0] for key, group in groupby(test_dataset.valid_entities, key=lambda x: x[0])]\n",
    "    valid_entity_per_basin = [\n",
    "        [i for _, i in group] for key, group in groupby(test_dataset.valid_entities, key=lambda x: x[0])\n",
    "    ]\n",
    "\n",
    "    # 3) Load trained model\n",
    "    model = load_model(RUN_FOLDER)\n",
    "    logging.info(\"Loaded trained model weights\")\n",
    "\n",
    "    # 4) Predict\n",
    "    test_results = {}\n",
    "    with torch.no_grad():\n",
    "        for i, (x_lstm, y) in enumerate(test_loader):\n",
    "            x_lstm = x_lstm.to(DEVICE)\n",
    "            y_sim = model(x_lstm)\n",
    "\n",
    "            # de-standardize model output (exactly like your script)\n",
    "            y_sim = y_sim * test_dataset.scaler[\"y_std\"].to(DEVICE) + test_dataset.scaler[\"y_mean\"].to(DEVICE)\n",
    "\n",
    "            basin_id = valid_basins[i]\n",
    "            df_ts = test_dataset.df_ts[basin_id].iloc[valid_entity_per_basin[i]]\n",
    "            df_new = pd.DataFrame(\n",
    "                data={\n",
    "                    \"y_obs\": y.flatten().cpu().numpy(),\n",
    "                    \"y_sim\": y_sim.flatten().cpu().numpy(),\n",
    "                },\n",
    "                index=df_ts.index,\n",
    "            )\n",
    "            test_results[basin_id] = df_new\n",
    "\n",
    "    # 5) Save outputs + NSE\n",
    "    out_pred = os.path.join(OUT_DIR, \"predictions\")\n",
    "    create_folder(out_pred)\n",
    "\n",
    "    # per-basin CSVs\n",
    "    for bid, df in test_results.items():\n",
    "        df.to_csv(os.path.join(out_pred, f\"{bid}_y_obs_y_sim.csv\"), index=True)\n",
    "\n",
    "    # NSE (per basin)\n",
    "    nse_vals = nse(df_results=test_results, average=False)\n",
    "    df_nse = pd.DataFrame({\"basin_id\": valid_basins, \"NSE\": np.round(nse_vals, 3)}).set_index(\"basin_id\")\n",
    "    df_nse.to_csv(os.path.join(OUT_DIR, \"NSE_one_more_testcase.csv\"))\n",
    "\n",
    "    # also write combined wide format like your script\n",
    "    y_sim = pd.concat([pd.DataFrame({k: v[\"y_sim\"]}) for k, v in test_results.items()], axis=1)\n",
    "    y_obs = pd.concat([pd.DataFrame({k: v[\"y_obs\"]}) for k, v in test_results.items()], axis=1)\n",
    "    y_sim.to_csv(os.path.join(OUT_DIR, \"y_sim_one_more_testcase.csv\"), index=True)\n",
    "    y_obs.to_csv(os.path.join(OUT_DIR, \"y_obs_one_more_testcase.csv\"), index=True)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    print(f\"Saved results in: {OUT_DIR}\")\n",
    "    print(df_nse)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edc55f16-76ad-49aa-85ce-950e55f92229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ensemble file: /hkfs/home/haicore/iwu/as2023/lstm_backward/results/with_discharge_haicore/ensemble_3seeds/one_more_testcase/predictions/neck_1_ensemble_y_obs_y_sim.csv\n",
      "Saved seed-matrix file: /hkfs/home/haicore/iwu/as2023/lstm_backward/results/with_discharge_haicore/ensemble_3seeds/one_more_testcase/predictions/neck_1_seeds_y_sim_matrix.csv\n",
      "            y_obs  y_sim_mean  y_sim_std  y_sim_min  y_sim_max\n",
      "1993-01-01    0.0    0.107385   0.101855   0.010355   0.213460\n",
      "1993-01-02    0.0    0.104652   0.105122   0.002297   0.212338\n",
      "1993-01-03    0.0    0.191113   0.130427   0.060833   0.321687\n",
      "1993-01-04    0.0    0.164880   0.186867  -0.048891   0.297178\n",
      "1993-01-05    0.0    0.242421   0.148036   0.078075   0.365304\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# EDIT THESE\n",
    "# -----------------------\n",
    "ROOT = Path(\"../results/with_discharge_haicore\")   # parent folder that contains seed_*/ dirs\n",
    "SEED_DIRS = [\"seed_47998\", \"seed_6697\", \"seed_77584\"]   # your 3 seeds\n",
    "BASIN_FILE = \"neck_1_y_obs_y_sim.csv\"              # file name inside predictions/\n",
    "REL_PATH = Path(\"one_more_testcase/predictions\")   # relative path inside each seed dir\n",
    "\n",
    "OUT_DIR = ROOT / \"ensemble_3seeds\" / \"one_more_testcase\" / \"predictions\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# Load per-seed predictions\n",
    "# -----------------------\n",
    "dfs = []\n",
    "for sd in SEED_DIRS:\n",
    "    f = ROOT / sd / REL_PATH / BASIN_FILE\n",
    "    if not f.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {f}\")\n",
    "    df = pd.read_csv(f, index_col=0, parse_dates=True).sort_index()\n",
    "    if \"y_sim\" not in df.columns:\n",
    "        raise ValueError(f\"{f} has no 'y_sim' column. Columns: {list(df.columns)}\")\n",
    "    dfs.append(df[[\"y_sim\"]].rename(columns={\"y_sim\": sd}))\n",
    "\n",
    "# -----------------------\n",
    "# Align on common timestamps\n",
    "# -----------------------\n",
    "common_index = dfs[0].index\n",
    "for df in dfs[1:]:\n",
    "    common_index = common_index.intersection(df.index)\n",
    "\n",
    "dfs = [df.loc[common_index] for df in dfs]\n",
    "\n",
    "# -----------------------\n",
    "# Ensemble stats\n",
    "# -----------------------\n",
    "y_sim_all = pd.concat(dfs, axis=1)  # columns = seed names\n",
    "y_sim_mean = y_sim_all.mean(axis=1).to_frame(\"y_sim_mean\")\n",
    "y_sim_std  = y_sim_all.std(axis=1).to_frame(\"y_sim_std\")\n",
    "y_sim_min  = y_sim_all.min(axis=1).to_frame(\"y_sim_min\")\n",
    "y_sim_max  = y_sim_all.max(axis=1).to_frame(\"y_sim_max\")\n",
    "\n",
    "# optionally also keep y_obs from the first seed (should be identical across seeds)\n",
    "f0 = ROOT / SEED_DIRS[0] / REL_PATH / BASIN_FILE\n",
    "y_obs = pd.read_csv(f0, index_col=0, parse_dates=True).sort_index().loc[common_index, [\"y_obs\"]]\n",
    "\n",
    "df_out = pd.concat([y_obs, y_sim_mean, y_sim_std, y_sim_min, y_sim_max], axis=1)\n",
    "\n",
    "# -----------------------\n",
    "# Save\n",
    "# -----------------------\n",
    "out_file = OUT_DIR / BASIN_FILE.replace(\"_y_obs_y_sim.csv\", \"_ensemble_y_obs_y_sim.csv\")\n",
    "df_out.to_csv(out_file)\n",
    "\n",
    "# also save the per-seed matrix (handy for debugging/plots)\n",
    "y_sim_all.to_csv(OUT_DIR / BASIN_FILE.replace(\"_y_obs_y_sim.csv\", \"_seeds_y_sim_matrix.csv\"))\n",
    "\n",
    "print(\"Saved ensemble file:\", out_file.resolve())\n",
    "print(\"Saved seed-matrix file:\", (OUT_DIR / BASIN_FILE.replace(\"_y_obs_y_sim.csv\", \"_seeds_y_sim_matrix.csv\")).resolve())\n",
    "print(df_out.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
